{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42cf36e-d7f2-4fd8-84f8-aded4cc9282d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edfb14a",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction with SparkML\n",
    "**EPITA – MSc Artificial Intelligence Systems (AIS)**  \n",
    "**Spark & Python for Big Data AIS S2 F25**\n",
    "\n",
    "**Students:** \n",
    "- TRUONG Kim Tan\n",
    "- LE Linh Long\n",
    "- George\n",
    "- Farouk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979118a9-9732-489b-80d1-db9ab01afba9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fad70e-9a8c-4be8-8899-7b8ad4e5d9af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Phase 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e170444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70db33-6677-459f-ba61-db3c586c998b",
   "metadata": {},
   "source": [
    "### 1.1. Iniialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aab44c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x11534dfd0>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ChurnPredictionPipeline\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57a659-e470-4884-be68-f8aae18b45ed",
   "metadata": {},
   "source": [
    "### 1.2. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0c8563d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\", nrows=1)\n",
    "cols = temp_df.columns.tolist()\n",
    "\n",
    "# build schema\n",
    "schema = StructType([StructField(c, StringType(), True) for c in cols])\n",
    "\n",
    "df = spark.read.csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaea6dc-0a09-4504-83c3-b2f20a518c21",
   "metadata": {},
   "source": [
    "### 1.3. Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "490814ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 7043\n",
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: string (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: string (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: string (nullable = true)\n",
      " |-- TotalCharges: string (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "|customerID|gender|SeniorCitizen|Partner|Dependents|tenure|PhoneService|   MultipleLines|InternetService|     OnlineSecurity|       OnlineBackup|   DeviceProtection|        TechSupport|        StreamingTV|    StreamingMovies|      Contract|PaperlessBilling|       PaymentMethod|MonthlyCharges|TotalCharges|Churn|\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "|7590-VHVEG|Female|            0|    Yes|        No|     1|          No|No phone service|            DSL|                 No|                Yes|                 No|                 No|                 No|                 No|Month-to-month|             Yes|    Electronic check|         29.85|       29.85|   No|\n",
      "|5575-GNVDE|  Male|            0|     No|        No|    34|         Yes|              No|            DSL|                Yes|                 No|                Yes|                 No|                 No|                 No|      One year|              No|        Mailed check|         56.95|      1889.5|   No|\n",
      "|3668-QPYBK|  Male|            0|     No|        No|     2|         Yes|              No|            DSL|                Yes|                Yes|                 No|                 No|                 No|                 No|Month-to-month|             Yes|        Mailed check|         53.85|      108.15|  Yes|\n",
      "|7795-CFOCW|  Male|            0|     No|        No|    45|          No|No phone service|            DSL|                Yes|                 No|                Yes|                Yes|                 No|                 No|      One year|              No|Bank transfer (au...|          42.3|     1840.75|   No|\n",
      "|9237-HQITU|Female|            0|     No|        No|     2|         Yes|              No|    Fiber optic|                 No|                 No|                 No|                 No|                 No|                 No|Month-to-month|             Yes|    Electronic check|          70.7|      151.65|  Yes|\n",
      "|9305-CDSKC|Female|            0|     No|        No|     8|         Yes|             Yes|    Fiber optic|                 No|                 No|                Yes|                 No|                Yes|                Yes|Month-to-month|             Yes|    Electronic check|         99.65|       820.5|  Yes|\n",
      "|1452-KIOVK|  Male|            0|     No|       Yes|    22|         Yes|             Yes|    Fiber optic|                 No|                Yes|                 No|                 No|                Yes|                 No|Month-to-month|             Yes|Credit card (auto...|          89.1|      1949.4|   No|\n",
      "|6713-OKOMC|Female|            0|     No|        No|    10|          No|No phone service|            DSL|                Yes|                 No|                 No|                 No|                 No|                 No|Month-to-month|              No|        Mailed check|         29.75|       301.9|   No|\n",
      "|7892-POOKP|Female|            0|    Yes|        No|    28|         Yes|             Yes|    Fiber optic|                 No|                 No|                Yes|                Yes|                Yes|                Yes|Month-to-month|             Yes|    Electronic check|         104.8|     3046.05|  Yes|\n",
      "|6388-TABGU|  Male|            0|     No|       Yes|    62|         Yes|              No|            DSL|                Yes|                Yes|                 No|                 No|                 No|                 No|      One year|              No|Bank transfer (au...|         56.15|     3487.95|   No|\n",
      "|9763-GRSKD|  Male|            0|    Yes|       Yes|    13|         Yes|              No|            DSL|                Yes|                 No|                 No|                 No|                 No|                 No|Month-to-month|             Yes|        Mailed check|         49.95|      587.45|   No|\n",
      "|7469-LKBCI|  Male|            0|     No|        No|    16|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      Two year|              No|Credit card (auto...|         18.95|       326.8|   No|\n",
      "|8091-TTVAX|  Male|            0|    Yes|        No|    58|         Yes|             Yes|    Fiber optic|                 No|                 No|                Yes|                 No|                Yes|                Yes|      One year|              No|Credit card (auto...|        100.35|      5681.1|   No|\n",
      "|0280-XJGEX|  Male|            0|     No|        No|    49|         Yes|             Yes|    Fiber optic|                 No|                Yes|                Yes|                 No|                Yes|                Yes|Month-to-month|             Yes|Bank transfer (au...|         103.7|      5036.3|  Yes|\n",
      "|5129-JLPIS|  Male|            0|     No|        No|    25|         Yes|              No|    Fiber optic|                Yes|                 No|                Yes|                Yes|                Yes|                Yes|Month-to-month|             Yes|    Electronic check|         105.5|     2686.05|   No|\n",
      "|3655-SNQYZ|Female|            0|    Yes|       Yes|    69|         Yes|             Yes|    Fiber optic|                Yes|                Yes|                Yes|                Yes|                Yes|                Yes|      Two year|              No|Credit card (auto...|        113.25|     7895.15|   No|\n",
      "|8191-XWSZG|Female|            0|     No|        No|    52|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      One year|              No|        Mailed check|         20.65|     1022.95|   No|\n",
      "|9959-WOFKT|  Male|            0|     No|       Yes|    71|         Yes|             Yes|    Fiber optic|                Yes|                 No|                Yes|                 No|                Yes|                Yes|      Two year|              No|Bank transfer (au...|         106.7|     7382.25|   No|\n",
      "|4190-MFLUW|Female|            0|    Yes|       Yes|    10|         Yes|              No|            DSL|                 No|                 No|                Yes|                Yes|                 No|                 No|Month-to-month|              No|Credit card (auto...|          55.2|      528.35|  Yes|\n",
      "|4183-MYFRB|Female|            0|     No|        No|    21|         Yes|              No|    Fiber optic|                 No|                Yes|                Yes|                 No|                 No|                Yes|Month-to-month|             Yes|    Electronic check|         90.05|      1862.9|   No|\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total rows: {df.count()}\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4262306c-3b00-4da8-9a19-0cc74d21f5d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0338e9e7",
   "metadata": {},
   "source": [
    "## Phase 2: Exploratory Data Analysis (EDA) & Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedcfe8a-1231-4cb3-918a-21941b008c5f",
   "metadata": {},
   "source": [
    "### 2.1. Data Cleaning (Handling Missing Values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a0de19-00c6-4f2a-88eb-ffb7fb2ca7fc",
   "metadata": {},
   "source": [
    "**a) Column count check for Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2d066f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column                 Missing Count\n",
      "----------------------------------------\n",
      "customerID                         0\n",
      "gender                             0\n",
      "SeniorCitizen                      0\n",
      "Partner                            0\n",
      "Dependents                         0\n",
      "tenure                             0\n",
      "PhoneService                       0\n",
      "MultipleLines                      0\n",
      "InternetService                    0\n",
      "OnlineSecurity                     0\n",
      "OnlineBackup                       0\n",
      "DeviceProtection                   0\n",
      "TechSupport                        0\n",
      "StreamingTV                        0\n",
      "StreamingMovies                    0\n",
      "Contract                           0\n",
      "PaperlessBilling                   0\n",
      "PaymentMethod                      0\n",
      "MonthlyCharges                     0\n",
      "TotalCharges                       0\n",
      "Churn                              0\n"
     ]
    }
   ],
   "source": [
    "def count_missing(c, dtype):\n",
    "    if isinstance(dtype, StringType):\n",
    "        return F.count(\n",
    "            F.when(\n",
    "                F.col(c).isNull() | (F.trim(F.col(c)) == \"\"), \n",
    "                c\n",
    "            )\n",
    "        ).alias(c)\n",
    "    else:\n",
    "        return F.count(F.when(F.col(c).isNull(), c)).alias(c)\n",
    "\n",
    "missing_counts = df.select([\n",
    "    count_missing(c, dtype) for c, dtype in df.dtypes\n",
    "])\n",
    "\n",
    "missing_data = missing_counts.collect()[0].asDict()\n",
    "\n",
    "print(f\"{'Column':<20} {'Missing Count':>15}\")\n",
    "print(\"-\" * 40)\n",
    "for col_name, count in missing_data.items():\n",
    "    print(f\"{col_name:<20} {count:>15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac05f2f-f929-496a-a1af-2af977412456",
   "metadata": {},
   "source": [
    "**b) Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9740a3ff-3e29-4f33-9141-f154cee82879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows before dropping null TotalCharges: 7043\n",
      "Rows after dropping null TotalCharges: 7032\n",
      "\n",
      "Duplicate customerID count: 0\n"
     ]
    }
   ],
   "source": [
    "# Handle TotalCharges: replace empty strings with null, then cast to Double\n",
    "df = df.withColumn('TotalCharges', \n",
    "                   F.when(F.col('TotalCharges').isin(\"\", \" \"), None)\n",
    "                   .otherwise(F.col('TotalCharges')))\n",
    "\n",
    "# Drop rows with null TotalCharges (11 records)\n",
    "print(f\"\\nRows before dropping null TotalCharges: {df.count()}\")\n",
    "df_clean = df.na.drop(subset=['TotalCharges'])\n",
    "print(f\"Rows after dropping null TotalCharges: {df_clean.count()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate customerID count: {df_clean.count() - df_clean.select('customerID').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0302dbf2-b787-41f7-b5a1-c148e539f487",
   "metadata": {},
   "source": [
    "### 2.2. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "96e4a41c-49b1-48f3-b799-98d5e88135ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema after conversion of columns:\n",
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: integer (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: double (nullable = true)\n",
      " |-- TotalCharges: double (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn('TotalCharges', F.col('TotalCharges').cast(DoubleType()))\n",
    "\n",
    "# Cast SeniorCitizen to Integer (here the values are currently \"0\", \"1\")\n",
    "df_clean = df_clean.withColumn('SeniorCitizen', F.col('SeniorCitizen').cast(IntegerType()))\n",
    "\n",
    "# Cast tenure to Integer\n",
    "df_clean = df_clean.withColumn('tenure', F.col('tenure').cast(IntegerType()))\n",
    "\n",
    "# Cast MonthlyCharges to Double\n",
    "df_clean = df_clean.withColumn('MonthlyCharges', F.col('MonthlyCharges').cast(DoubleType()))\n",
    "\n",
    "print(\"Schema after conversion of columns:\")\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58c3d1-71a4-4c32-9a4b-c9b7c050940b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.3. Univariate Analysis (Analyzing Single Variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c863665e-dde4-4044-ad92-f5449726c772",
   "metadata": {},
   "source": [
    "**a) Numerical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a19aa974-ae01-439a-937a-5ca3f92f7684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|            tenure|    MonthlyCharges|      TotalCharges|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|              7032|              7032|              7032|\n",
      "|   mean|32.421786120591584| 64.79820819112632|2283.3004408418697|\n",
      "| stddev|24.545259709263245|30.085973884049825| 2266.771361883145|\n",
      "|    min|                 1|             18.25|              18.8|\n",
      "|    max|                72|            118.75|            8684.8|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n",
      "SeniorCitizen Distribution:\n",
      "+-------------+-----+\n",
      "|SeniorCitizen|count|\n",
      "+-------------+-----+\n",
      "|            0| 5890|\n",
      "|            1| 1142|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# numerical columns summary\n",
    "numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "df_clean.select(numerical_cols).describe().show()\n",
    "\n",
    "# SeniorCitizen belongs to categorical column\n",
    "print(\"SeniorCitizen Distribution:\")\n",
    "df_clean.groupBy('SeniorCitizen').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc24abb-5ea9-4e97-8a9c-e8ba7d044995",
   "metadata": {},
   "source": [
    "**b) Categorical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8cade52b-75d4-4415-bb34-ee517dcf9a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn']\n",
      "gender distribution:\n",
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|  Male| 3549|\n",
      "|Female| 3483|\n",
      "+------+-----+\n",
      "\n",
      "Partner distribution:\n",
      "+-------+-----+\n",
      "|Partner|count|\n",
      "+-------+-----+\n",
      "|     No| 3639|\n",
      "|    Yes| 3393|\n",
      "+-------+-----+\n",
      "\n",
      "Dependents distribution:\n",
      "+----------+-----+\n",
      "|Dependents|count|\n",
      "+----------+-----+\n",
      "|        No| 4933|\n",
      "|       Yes| 2099|\n",
      "+----------+-----+\n",
      "\n",
      "Contract distribution:\n",
      "+--------------+-----+\n",
      "|      Contract|count|\n",
      "+--------------+-----+\n",
      "|Month-to-month| 3875|\n",
      "|      Two year| 1685|\n",
      "|      One year| 1472|\n",
      "+--------------+-----+\n",
      "\n",
      "InternetService distribution:\n",
      "+---------------+-----+\n",
      "|InternetService|count|\n",
      "+---------------+-----+\n",
      "|    Fiber optic| 3096|\n",
      "|            DSL| 2416|\n",
      "|             No| 1520|\n",
      "+---------------+-----+\n",
      "\n",
      "PaymentMethod distribution:\n",
      "+--------------------+-----+\n",
      "|       PaymentMethod|count|\n",
      "+--------------------+-----+\n",
      "|    Electronic check| 2365|\n",
      "|        Mailed check| 1604|\n",
      "|Bank transfer (au...| 1542|\n",
      "|Credit card (auto...| 1521|\n",
      "+--------------------+-----+\n",
      "\n",
      "Churn distribution:\n",
      "+-----+-----+\n",
      "|Churn|count|\n",
      "+-----+-----+\n",
      "|   No| 5163|\n",
      "|  Yes| 1869|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [c for c in df_clean.columns if c not in numerical_cols + ['customerID', 'SeniorCitizen']]\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Show value counts for key categorical columns\n",
    "key_categorical = ['gender', 'Partner', 'Dependents', 'Contract', \n",
    "                   'InternetService', 'PaymentMethod', 'Churn']\n",
    "\n",
    "for col in key_categorical:\n",
    "    print(f\"{col} distribution:\")\n",
    "    df_clean.groupBy(col).count().orderBy(F.desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459e000b-1247-429f-adfc-64e2d15bd030",
   "metadata": {},
   "source": [
    "### 2.4. Bivariate Analysis (Analyzing Relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "107bc124-282d-4567-aef2-a2012be1716a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Churn rate by Contract type:\n",
      "+--------------+-----+-------+------------------+\n",
      "|      Contract|total|churned|        churn_rate|\n",
      "+--------------+-----+-------+------------------+\n",
      "|Month-to-month| 3875|   1655| 42.70967741935484|\n",
      "|      One year| 1472|    166|11.277173913043478|\n",
      "|      Two year| 1685|     48|2.8486646884272995|\n",
      "+--------------+-----+-------+------------------+\n",
      "\n",
      "Churn rate by Internet Service:\n",
      "+---------------+-----+-------+------------------+\n",
      "|InternetService|total|churned|        churn_rate|\n",
      "+---------------+-----+-------+------------------+\n",
      "|             No| 1520|    113| 7.434210526315789|\n",
      "|            DSL| 2416|    459|18.998344370860927|\n",
      "|    Fiber optic| 3096|   1297| 41.89276485788114|\n",
      "+---------------+-----+-------+------------------+\n",
      "\n",
      "Tenure statistics by Churn:\n",
      "+-----+------------------+----------+----------+-------------------+------------------+\n",
      "|Churn|        avg_tenure|min_tenure|max_tenure|avg_monthly_charges| avg_total_charges|\n",
      "+-----+------------------+----------+----------+-------------------+------------------+\n",
      "|   No| 37.65000968429208|         1|        72|  61.30740848343966|2555.3441410032997|\n",
      "|  Yes|17.979133226324237|         1|        72|   74.4413322632423|1531.7960941680035|\n",
      "+-----+------------------+----------+----------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Churn rate by Contract type:\")\n",
    "df_clean.groupBy('Contract').agg(\n",
    "    F.count('*').alias('total'),\n",
    "    F.sum(F.when(F.col('Churn') == 'Yes', 1).otherwise(0)).alias('churned'),\n",
    "    (F.sum(F.when(F.col('Churn') == 'Yes', 1).otherwise(0)) / F.count('*') * 100).alias('churn_rate')\n",
    ").show()\n",
    "\n",
    "print(\"Churn rate by Internet Service:\")\n",
    "df_clean.groupBy('InternetService').agg(\n",
    "    F.count('*').alias('total'),\n",
    "    F.sum(F.when(F.col('Churn') == 'Yes', 1).otherwise(0)).alias('churned'),\n",
    "    (F.sum(F.when(F.col('Churn') == 'Yes', 1).otherwise(0)) / F.count('*') * 100).alias('churn_rate')\n",
    ").show()\n",
    "\n",
    "# Tenure statistics by Churn\n",
    "print(\"Tenure statistics by Churn:\")\n",
    "df_clean.groupBy('Churn').agg(\n",
    "    F.avg('tenure').alias('avg_tenure'),\n",
    "    F.min('tenure').alias('min_tenure'),\n",
    "    F.max('tenure').alias('max_tenure'),\n",
    "    F.avg('MonthlyCharges').alias('avg_monthly_charges'),\n",
    "    F.avg('TotalCharges').alias('avg_total_charges')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8427373-cc99-44c4-a504-cd7efac4797e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaeca74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Phase 3: Data Transformation & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3484e57-e344-4ef7-aa64-c9b5f2fa8ce1",
   "metadata": {},
   "source": [
    "### 3.1. Identify Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e470db35-b391-4dfc-a865-e68432cf27f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to use: ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges']\n",
      "Categorical: ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
      "Numerical: ['tenure', 'MonthlyCharges', 'TotalCharges']\n"
     ]
    }
   ],
   "source": [
    "# we identify feature columns (except for customerID and target)\n",
    "feature_cols = [c for c in df_clean.columns if c not in ['customerID', 'Churn']]\n",
    "\n",
    "# we separate columns into categorical and numerical\n",
    "cat_cols = [c for c in feature_cols if c not in numerical_cols]\n",
    "num_cols = [c for c in feature_cols if c in numerical_cols]\n",
    "\n",
    "print(f\"Features to use: {feature_cols}\")\n",
    "print(f\"Categorical: {cat_cols}\")\n",
    "print(f\"Numerical: {num_cols}\")\n",
    "\n",
    "# we convert the target variable to numeric\n",
    "df_clean = df_clean.withColumn('label', F.when(F.col('Churn') == 'Yes', 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172a3f2-7f40-4364-8010-9fdb2b66faca",
   "metadata": {},
   "source": [
    "### 3.2. Define Pipeline Stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d8745-27da-4547-9f6b-e6686b7fbd4b",
   "metadata": {},
   "source": [
    "**a) Categorical Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a0bbd092-f407-42b7-a67d-4cd2791b96a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = []\n",
    "encoders = []\n",
    "indexed_cols = []\n",
    "encoded_cols = []\n",
    "\n",
    "for col in cat_cols:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\", handleInvalid='keep')\n",
    "    encoder = OneHotEncoder(inputCol=f\"{col}_indexed\", outputCol=f\"{col}_encoded\", dropLast=False)\n",
    "    indexers.append(indexer)\n",
    "    encoders.append(encoder)\n",
    "    indexed_cols.append(f\"{col}_indexed\")\n",
    "    encoded_cols.append(f\"{col}_encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001134f8-9cec-47d6-89ed-b819be7cec39",
   "metadata": {},
   "source": [
    "**b) Vector Assembly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "de497752-e1e5-4f94-89e3-5401a61e874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine numerical features with encoded categorical features\n",
    "assembler_inputs = num_cols + encoded_cols\n",
    "vector_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol='features_unscaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb2b765-3e55-44ce-96ba-b54ad6fba7f3",
   "metadata": {},
   "source": [
    "**c) Feature Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "08e5b918-4023-4588-a08a-d2a4b8662922",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features_unscaled', outputCol='features', \n",
    "                        withStd=True, withMean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150031ad-ccc6-461a-8f80-ede664f94e00",
   "metadata": {},
   "source": [
    "**d) Create Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "823b0533-c664-455e-ab0b-5357dc6aaba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline stages: 16 indexers, 16 encoders, 1 assembler, 1 scaler\n",
      "Total stages in pipeline: 34\n"
     ]
    }
   ],
   "source": [
    "pipeline_stages = indexers + encoders + [vector_assembler, scaler]\n",
    "pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "print(f\"Pipeline stages: {len(indexers)} indexers, {len(encoders)} encoders, 1 assembler, 1 scaler\")\n",
    "print(f\"Total stages in pipeline: {len(pipeline_stages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cbf5db-7ff8-4d7c-bbe1-f32635386fc0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604e5e90",
   "metadata": {},
   "source": [
    "## Phase 4: Linear Regression Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph4-md-lr",
   "metadata": {},
   "source": [
    "### 4.1. Define Logistic Regression & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ph4-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='label'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph4-md-pipe",
   "metadata": {},
   "source": [
    "### 4.2. Build Full Pipeline (Preprocessing + LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ph4-pipe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR pipeline created with the following stages:\n",
      "  1. StringIndexer\n",
      "  2. StringIndexer\n",
      "  3. StringIndexer\n",
      "  4. StringIndexer\n",
      "  5. StringIndexer\n",
      "  6. StringIndexer\n",
      "  7. StringIndexer\n",
      "  8. StringIndexer\n",
      "  9. StringIndexer\n",
      "  10. StringIndexer\n",
      "  11. StringIndexer\n",
      "  12. StringIndexer\n",
      "  13. StringIndexer\n",
      "  14. StringIndexer\n",
      "  15. StringIndexer\n",
      "  16. StringIndexer\n",
      "  17. OneHotEncoder\n",
      "  18. OneHotEncoder\n",
      "  19. OneHotEncoder\n",
      "  20. OneHotEncoder\n",
      "  21. OneHotEncoder\n",
      "  22. OneHotEncoder\n",
      "  23. OneHotEncoder\n",
      "  24. OneHotEncoder\n",
      "  25. OneHotEncoder\n",
      "  26. OneHotEncoder\n",
      "  27. OneHotEncoder\n",
      "  28. OneHotEncoder\n",
      "  29. OneHotEncoder\n",
      "  30. OneHotEncoder\n",
      "  31. OneHotEncoder\n",
      "  32. OneHotEncoder\n",
      "  33. VectorAssembler\n",
      "  34. StandardScaler\n",
      "  35. LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "lr_pipeline = Pipeline(stages=indexers + encoders + [vector_assembler, scaler, lr])\n",
    "\n",
    "print('LR pipeline created with the following stages:')\n",
    "for i, stage in enumerate(lr_pipeline.getStages()):\n",
    "    print(f'  {i+1}. {stage.__class__.__name__}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph4-md-split",
   "metadata": {},
   "source": [
    "### 4.3. Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ph4-split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set : 5690 records\n",
      "Test set     : 1342 records\n",
      "\n",
      "Class distribution in training set:\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0| 4175|\n",
      "|    1| 1515|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f'Training set : {train_df.count()} records')\n",
    "print(f'Test set     : {test_df.count()} records')\n",
    "print('\\nClass distribution in training set:')\n",
    "train_df.groupBy('label').count().orderBy('label').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph4-md-grid",
   "metadata": {},
   "source": [
    "### 4.4. Define Hyperparameter Grid with ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ph4-grid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR ParamGrid built — 12 combinations\n",
      "  Combo  1: {'regParam': 0.01, 'elasticNetParam': 0.0, 'maxIter': 10}\n",
      "  Combo  2: {'regParam': 0.01, 'elasticNetParam': 0.0, 'maxIter': 50}\n",
      "  Combo  3: {'regParam': 0.01, 'elasticNetParam': 0.5, 'maxIter': 10}\n",
      "  Combo  4: {'regParam': 0.01, 'elasticNetParam': 0.5, 'maxIter': 50}\n",
      "  Combo  5: {'regParam': 0.1, 'elasticNetParam': 0.0, 'maxIter': 10}\n",
      "  Combo  6: {'regParam': 0.1, 'elasticNetParam': 0.0, 'maxIter': 50}\n",
      "  Combo  7: {'regParam': 0.1, 'elasticNetParam': 0.5, 'maxIter': 10}\n",
      "  Combo  8: {'regParam': 0.1, 'elasticNetParam': 0.5, 'maxIter': 50}\n",
      "  Combo  9: {'regParam': 0.5, 'elasticNetParam': 0.0, 'maxIter': 10}\n",
      "  Combo 10: {'regParam': 0.5, 'elasticNetParam': 0.0, 'maxIter': 50}\n",
      "  Combo 11: {'regParam': 0.5, 'elasticNetParam': 0.5, 'maxIter': 10}\n",
      "  Combo 12: {'regParam': 0.5, 'elasticNetParam': 0.5, 'maxIter': 50}\n"
     ]
    }
   ],
   "source": [
    "lr_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.regParam,        [0.01, 0.1, 0.5])\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5])\n",
    "    .addGrid(lr.maxIter,         [10, 50])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(f'LR ParamGrid built — {len(lr_param_grid)} combinations')\n",
    "for i, params in enumerate(lr_param_grid):\n",
    "    combo = {p.name: v for p, v in params.items()}\n",
    "    print(f'  Combo {i+1:>2}: {combo}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph4-md-cv",
   "metadata": {},
   "source": [
    "### 4.5. Cross-Validation Setup (5-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ph4-cv",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol='label',\n",
    "    rawPredictionCol='rawPrediction',\n",
    "    metricName='areaUnderROC'\n",
    ")\n",
    "\n",
    "lr_cross_val = CrossValidator(\n",
    "    estimator=lr_pipeline,\n",
    "    estimatorParamMaps=lr_param_grid,\n",
    "    evaluator=cv_evaluator,\n",
    "    numFolds=5,\n",
    "    seed=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph4-md-train",
   "metadata": {},
   "source": [
    "### 4.6. Train with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ph4-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average AUC-ROC per hyperparameter combination:\n",
      "Combo    regParam   elasticNet   maxIter    AUC-ROC\n",
      "-----------------------------------------------------\n",
      "1        0.01       0.0          10          0.8389\n",
      "2        0.01       0.0          50          0.8397\n",
      "3        0.01       0.5          10          0.8401\n",
      "4        0.01       0.5          50          0.8404\n",
      "5        0.1        0.0          10          0.8367\n",
      "6        0.1        0.0          50          0.8367\n",
      "7        0.1        0.5          10          0.8334\n",
      "8        0.1        0.5          50          0.8332\n",
      "9        0.5        0.0          10          0.8301\n",
      "10       0.5        0.0          50          0.8301\n",
      "11       0.5        0.5          10          0.5000\n",
      "12       0.5        0.5          50          0.5000\n",
      "\n",
      "Best combo #4: {'regParam': 0.01, 'elasticNetParam': 0.5, 'maxIter': 50}  →  AUC = 0.8404\n"
     ]
    }
   ],
   "source": [
    "lr_cv_model = lr_cross_val.fit(train_df)\n",
    "\n",
    "print('\\nAverage AUC-ROC per hyperparameter combination:')\n",
    "print(f\"{'Combo':<8} {'regParam':<10} {'elasticNet':<12} {'maxIter':<9} {'AUC-ROC':>8}\")\n",
    "print('-' * 53)\n",
    "for i, (params, score) in enumerate(zip(lr_param_grid, lr_cv_model.avgMetrics)):\n",
    "    combo = {p.name: v for p, v in params.items()}\n",
    "    print(f\"{i+1:<8} {combo['regParam']:<10} {combo['elasticNetParam']:<12} {combo['maxIter']:<9} {score:>8.4f}\")\n",
    "\n",
    "best_lr_score  = max(lr_cv_model.avgMetrics)\n",
    "best_lr_idx    = lr_cv_model.avgMetrics.index(best_lr_score)\n",
    "best_lr_params = {p.name: v for p, v in lr_param_grid[best_lr_idx].items()}\n",
    "print(f'\\nBest combo #{best_lr_idx+1}: {best_lr_params}  →  AUC = {best_lr_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph4-md-eval",
   "metadata": {},
   "source": [
    "### 4.7. Evaluate Best Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ph4-eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Evaluation\n",
      "----------------------------------------\n",
      "  Accuracy          : 0.8137\n",
      "  F1 Score          : 0.8050\n",
      "  Weighted Precision: 0.8042\n",
      "  Weighted Recall   : 0.8137\n",
      "  AUC-ROC           : 0.8555\n",
      "\n",
      "Confusion Matrix:\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0|  905|\n",
      "|    0|       1.0|   83|\n",
      "|    1|       0.0|  167|\n",
      "|    1|       1.0|  187|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_cv_model.transform(test_df)\n",
    "\n",
    "lr_acc = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy').evaluate(lr_predictions)\n",
    "lr_f1 = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1').evaluate(lr_predictions)\n",
    "lr_prec = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='weightedPrecision').evaluate(lr_predictions)\n",
    "lr_rec = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='weightedRecall').evaluate(lr_predictions)\n",
    "lr_auc = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderROC').evaluate(lr_predictions)\n",
    "\n",
    "print(\"\\nLogistic Regression Evaluation\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Accuracy          : {lr_acc:.4f}\")\n",
    "print(f\"  F1 Score          : {lr_f1:.4f}\")\n",
    "print(f\"  Weighted Precision: {lr_prec:.4f}\")\n",
    "print(f\"  Weighted Recall   : {lr_rec:.4f}\")\n",
    "print(f\"  AUC-ROC           : {lr_auc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "lr_predictions.groupBy('label', 'prediction').count().orderBy('label', 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph5-sep",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph5-title",
   "metadata": {},
   "source": [
    "## Phase 5: Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph5-md-import",
   "metadata": {},
   "source": [
    "### 5.1. Add RandomForestClassifier to the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ph5-import",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF pipeline created with the following stages:\n",
      "  1. StringIndexer\n",
      "  2. StringIndexer\n",
      "  3. StringIndexer\n",
      "  4. StringIndexer\n",
      "  5. StringIndexer\n",
      "  6. StringIndexer\n",
      "  7. StringIndexer\n",
      "  8. StringIndexer\n",
      "  9. StringIndexer\n",
      "  10. StringIndexer\n",
      "  11. StringIndexer\n",
      "  12. StringIndexer\n",
      "  13. StringIndexer\n",
      "  14. StringIndexer\n",
      "  15. StringIndexer\n",
      "  16. StringIndexer\n",
      "  17. OneHotEncoder\n",
      "  18. OneHotEncoder\n",
      "  19. OneHotEncoder\n",
      "  20. OneHotEncoder\n",
      "  21. OneHotEncoder\n",
      "  22. OneHotEncoder\n",
      "  23. OneHotEncoder\n",
      "  24. OneHotEncoder\n",
      "  25. OneHotEncoder\n",
      "  26. OneHotEncoder\n",
      "  27. OneHotEncoder\n",
      "  28. OneHotEncoder\n",
      "  29. OneHotEncoder\n",
      "  30. OneHotEncoder\n",
      "  31. OneHotEncoder\n",
      "  32. OneHotEncoder\n",
      "  33. VectorAssembler\n",
      "  34. StandardScaler\n",
      "  35. RandomForestClassifier\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_pipeline = Pipeline(stages=indexers + encoders + [vector_assembler, scaler, rf])\n",
    "\n",
    "print(\"RF pipeline created with the following stages:\")\n",
    "for i, stage in enumerate(rf_pipeline.getStages()):\n",
    "    print(f\"  {i+1}. {stage.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph5-md-grid",
   "metadata": {},
   "source": [
    "### 5.2. Define Hyperparameter Grid with ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ph5-grid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParamGrid built — 12 hyperparameter combinations to evaluate\n",
      "  Combo  1: {'numTrees': 50, 'maxDepth': 4, 'minInstancesPerNode': 1}\n",
      "  Combo  2: {'numTrees': 50, 'maxDepth': 4, 'minInstancesPerNode': 2}\n",
      "  Combo  3: {'numTrees': 50, 'maxDepth': 6, 'minInstancesPerNode': 1}\n",
      "  Combo  4: {'numTrees': 50, 'maxDepth': 6, 'minInstancesPerNode': 2}\n",
      "  Combo  5: {'numTrees': 50, 'maxDepth': 8, 'minInstancesPerNode': 1}\n",
      "  Combo  6: {'numTrees': 50, 'maxDepth': 8, 'minInstancesPerNode': 2}\n",
      "  Combo  7: {'numTrees': 70, 'maxDepth': 4, 'minInstancesPerNode': 1}\n",
      "  Combo  8: {'numTrees': 70, 'maxDepth': 4, 'minInstancesPerNode': 2}\n",
      "  Combo  9: {'numTrees': 70, 'maxDepth': 6, 'minInstancesPerNode': 1}\n",
      "  Combo 10: {'numTrees': 70, 'maxDepth': 6, 'minInstancesPerNode': 2}\n",
      "  Combo 11: {'numTrees': 70, 'maxDepth': 8, 'minInstancesPerNode': 1}\n",
      "  Combo 12: {'numTrees': 70, 'maxDepth': 8, 'minInstancesPerNode': 2}\n"
     ]
    }
   ],
   "source": [
    "rf_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees,             [50, 70])\n",
    "    .addGrid(rf.maxDepth,             [4, 6, 8])\n",
    "    .addGrid(rf.minInstancesPerNode,  [1, 2])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(f\"ParamGrid built — {len(rf_param_grid)} hyperparameter combinations to evaluate\")\n",
    "for i, params in enumerate(rf_param_grid):\n",
    "    combo = {p.name: v for p, v in params.items()}\n",
    "    print(f\"  Combo {i+1:>2}: {combo}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph5-md-cv",
   "metadata": {},
   "source": [
    "### 5.3. Cross-Validation Setup (5-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ph5-cv",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cross_val = CrossValidator(\n",
    "    estimator=rf_pipeline,\n",
    "    estimatorParamMaps=rf_param_grid,\n",
    "    evaluator=cv_evaluator,\n",
    "    numFolds=5,\n",
    "    seed=42,\n",
    "    parallelism=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph5-md-train",
   "metadata": {},
   "source": [
    "### 5.4. Train with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ph5-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/22 21:29:15 WARN DAGScheduler: Broadcasting large task binary with size 1028.3 KiB\n",
      "26/02/22 21:29:15 WARN DAGScheduler: Broadcasting large task binary with size 1572.8 KiB\n",
      "26/02/22 21:29:17 WARN DAGScheduler: Broadcasting large task binary with size 1028.0 KiB\n",
      "26/02/22 21:29:17 WARN DAGScheduler: Broadcasting large task binary with size 1562.9 KiB\n",
      "26/02/22 21:29:25 WARN DAGScheduler: Broadcasting large task binary with size 1341.4 KiB\n",
      "26/02/22 21:29:26 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "26/02/22 21:29:26 WARN DAGScheduler: Broadcasting large task binary with size 1281.4 KiB\n",
      "26/02/22 21:29:28 WARN DAGScheduler: Broadcasting large task binary with size 1330.5 KiB\n",
      "26/02/22 21:29:28 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "26/02/22 21:29:28 WARN DAGScheduler: Broadcasting large task binary with size 1175.7 KiB\n",
      "26/02/22 21:29:36 WARN DAGScheduler: Broadcasting large task binary with size 1031.1 KiB\n",
      "26/02/22 21:29:36 WARN DAGScheduler: Broadcasting large task binary with size 1581.4 KiB\n",
      "26/02/22 21:29:38 WARN DAGScheduler: Broadcasting large task binary with size 1023.8 KiB\n",
      "26/02/22 21:29:38 WARN DAGScheduler: Broadcasting large task binary with size 1565.3 KiB\n",
      "26/02/22 21:29:47 WARN DAGScheduler: Broadcasting large task binary with size 1335.4 KiB\n",
      "26/02/22 21:29:47 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "26/02/22 21:29:48 WARN DAGScheduler: Broadcasting large task binary with size 1250.7 KiB\n",
      "26/02/22 21:29:49 WARN DAGScheduler: Broadcasting large task binary with size 1333.8 KiB\n",
      "26/02/22 21:29:49 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "26/02/22 21:29:50 WARN DAGScheduler: Broadcasting large task binary with size 1178.6 KiB\n",
      "26/02/22 21:29:57 WARN DAGScheduler: Broadcasting large task binary with size 1024.6 KiB\n",
      "26/02/22 21:29:57 WARN DAGScheduler: Broadcasting large task binary with size 1574.1 KiB\n",
      "26/02/22 21:29:59 WARN DAGScheduler: Broadcasting large task binary with size 1022.8 KiB\n",
      "26/02/22 21:29:59 WARN DAGScheduler: Broadcasting large task binary with size 1566.6 KiB\n",
      "26/02/22 21:30:08 WARN DAGScheduler: Broadcasting large task binary with size 1339.9 KiB\n",
      "26/02/22 21:30:08 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "26/02/22 21:30:09 WARN DAGScheduler: Broadcasting large task binary with size 1262.0 KiB\n",
      "26/02/22 21:30:10 WARN DAGScheduler: Broadcasting large task binary with size 1336.0 KiB\n",
      "26/02/22 21:30:11 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "26/02/22 21:30:11 WARN DAGScheduler: Broadcasting large task binary with size 1157.3 KiB\n",
      "26/02/22 21:30:19 WARN DAGScheduler: Broadcasting large task binary with size 1025.5 KiB\n",
      "26/02/22 21:30:19 WARN DAGScheduler: Broadcasting large task binary with size 1574.2 KiB\n",
      "26/02/22 21:30:21 WARN DAGScheduler: Broadcasting large task binary with size 1027.3 KiB\n",
      "26/02/22 21:30:21 WARN DAGScheduler: Broadcasting large task binary with size 1571.1 KiB\n",
      "26/02/22 21:30:30 WARN DAGScheduler: Broadcasting large task binary with size 1350.5 KiB\n",
      "26/02/22 21:30:30 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "26/02/22 21:30:31 WARN DAGScheduler: Broadcasting large task binary with size 1287.0 KiB\n",
      "26/02/22 21:30:32 WARN DAGScheduler: Broadcasting large task binary with size 1351.4 KiB\n",
      "26/02/22 21:30:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "26/02/22 21:30:33 WARN DAGScheduler: Broadcasting large task binary with size 1175.8 KiB\n",
      "26/02/22 21:30:40 WARN DAGScheduler: Broadcasting large task binary with size 1027.7 KiB\n",
      "26/02/22 21:30:40 WARN DAGScheduler: Broadcasting large task binary with size 1587.0 KiB\n",
      "26/02/22 21:30:42 WARN DAGScheduler: Broadcasting large task binary with size 1020.8 KiB\n",
      "26/02/22 21:30:42 WARN DAGScheduler: Broadcasting large task binary with size 1563.7 KiB\n",
      "26/02/22 21:30:51 WARN DAGScheduler: Broadcasting large task binary with size 1324.7 KiB\n",
      "26/02/22 21:30:51 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "26/02/22 21:30:52 WARN DAGScheduler: Broadcasting large task binary with size 1267.5 KiB\n",
      "26/02/22 21:30:53 WARN DAGScheduler: Broadcasting large task binary with size 1324.8 KiB\n",
      "26/02/22 21:30:53 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "26/02/22 21:30:54 WARN DAGScheduler: Broadcasting large task binary with size 1175.8 KiB\n",
      "26/02/22 21:30:57 WARN DAGScheduler: Broadcasting large task binary with size 1348.5 KiB\n",
      "26/02/22 21:30:57 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average AUC-ROC per hyperparameter combination:\n",
      "Combo    numTrees   maxDepth   minInst   AUC-ROC\n",
      "--------------------------------------------------\n",
      "1        50         4          1          0.8308\n",
      "2        50         4          2          0.8311\n",
      "3        50         6          1          0.8386\n",
      "4        50         6          2          0.8383\n",
      "5        50         8          1          0.8401\n",
      "6        50         8          2          0.8404\n",
      "7        70         4          1          0.8310\n",
      "8        70         4          2          0.8308\n",
      "9        70         6          1          0.8380\n",
      "10       70         6          2          0.8380\n",
      "11       70         8          1          0.8406\n",
      "12       70         8          2          0.8402\n",
      "\n",
      "Best combo #11: {'numTrees': 70, 'maxDepth': 8, 'minInstancesPerNode': 1}  →  AUC = 0.8406\n"
     ]
    }
   ],
   "source": [
    "rf_cv_model = rf_cross_val.fit(train_df)\n",
    "\n",
    "print(\"\\nAverage AUC-ROC per hyperparameter combination:\")\n",
    "print(f\"{'Combo':<8} {'numTrees':<10} {'maxDepth':<10} {'minInst':<8} {'AUC-ROC':>8}\")\n",
    "print(\"-\" * 50)\n",
    "for i, (params, score) in enumerate(zip(rf_param_grid, rf_cv_model.avgMetrics)):\n",
    "    combo = {p.name: v for p, v in params.items()}\n",
    "    print(f\"{i+1:<8} {combo['numTrees']:<10} {combo['maxDepth']:<10} {combo['minInstancesPerNode']:<8} {score:>8.4f}\")\n",
    "\n",
    "best_rf_score = max(rf_cv_model.avgMetrics)\n",
    "best_rf_idx   = rf_cv_model.avgMetrics.index(best_rf_score)\n",
    "best_rf_params = {p.name: v for p, v in rf_param_grid[best_rf_idx].items()}\n",
    "print(f\"\\nBest combo #{best_rf_idx+1}: {best_rf_params}  →  AUC = {best_rf_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph5-md-eval",
   "metadata": {},
   "source": [
    "### 5.5. Evaluate Best Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ph5-eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/22 21:30:58 WARN DAGScheduler: Broadcasting large task binary with size 1272.7 KiB\n",
      "26/02/22 21:30:58 WARN DAGScheduler: Broadcasting large task binary with size 1272.7 KiB\n",
      "26/02/22 21:30:58 WARN DAGScheduler: Broadcasting large task binary with size 1272.7 KiB\n",
      "26/02/22 21:30:58 WARN DAGScheduler: Broadcasting large task binary with size 1272.7 KiB\n",
      "26/02/22 21:30:58 WARN DAGScheduler: Broadcasting large task binary with size 1261.0 KiB\n",
      "26/02/22 21:30:58 WARN DAGScheduler: Broadcasting large task binary with size 1268.7 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Evaluation\n",
      "----------------------------------------\n",
      "  Accuracy          : 0.8040\n",
      "  F1 Score          : 0.7916\n",
      "  Weighted Precision: 0.7923\n",
      "  Weighted Recall   : 0.8040\n",
      "  AUC-ROC           : 0.8557\n",
      "\n",
      "Confusion Matrix:\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0|  910|\n",
      "|    0|       1.0|   78|\n",
      "|    1|       0.0|  185|\n",
      "|    1|       1.0|  169|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/22 21:30:58 WARN DAGScheduler: Broadcasting large task binary with size 1226.7 KiB\n"
     ]
    }
   ],
   "source": [
    "rf_predictions = rf_cv_model.transform(test_df)\n",
    "\n",
    "rf_acc = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy').evaluate(rf_predictions)\n",
    "rf_f1 = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1').evaluate(rf_predictions)\n",
    "rf_prec = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='weightedPrecision').evaluate(rf_predictions)\n",
    "rf_rec = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='weightedRecall').evaluate(rf_predictions)\n",
    "rf_auc = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderROC').evaluate(rf_predictions)\n",
    "\n",
    "print(\"\\nRandom Forest Evaluation\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Accuracy          : {rf_acc:.4f}\")\n",
    "print(f\"  F1 Score          : {rf_f1:.4f}\")\n",
    "print(f\"  Weighted Precision: {rf_prec:.4f}\")\n",
    "print(f\"  Weighted Recall   : {rf_rec:.4f}\")\n",
    "print(f\"  AUC-ROC           : {rf_auc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "rf_predictions.groupBy('label', 'prediction').count().orderBy('label', 'prediction').show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
