{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42cf36e-d7f2-4fd8-84f8-aded4cc9282d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edfb14a",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction with SparkML\n",
    "**EPITA – MSc Artificial Intelligence Systems (AIS)**  \n",
    "**Spark & Python for Big Data AIS S2 F25**\n",
    "\n",
    "**Students:** \n",
    "- TRUONG Kim Tan\n",
    "- LE Linh Long\n",
    "- George\n",
    "- Farouk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979118a9-9732-489b-80d1-db9ab01afba9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fad70e-9a8c-4be8-8899-7b8ad4e5d9af",
   "metadata": {},
   "source": [
    "## Phase 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc2c58ee-9211-411d-950f-39e55053ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e170444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27d604b9-15b8-4e73-896d-be1a15190591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70db33-6677-459f-ba61-db3c586c998b",
   "metadata": {},
   "source": [
    "### 1.1. Iniialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aab44c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000019FDFDEC370>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ChurnPredictionPipeline\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57a659-e470-4884-be68-f8aae18b45ed",
   "metadata": {},
   "source": [
    "### 1.2. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c8563d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: long (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: long (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: double (nullable = true)\n",
      " |-- TotalCharges: string (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df = pd.read_csv(\"C:/Users/pc/Desktop/Epita Msc AIS 1/AIS S2/Spark and python for big data/project 1/customer-churn-sparkml/WA_Fn-UseC_-Telco-Customer-Churn.csv\", nrows=1)\n",
    "cols = temp_df.columns.tolist()\n",
    "\n",
    "# build schema\n",
    "schema = StructType([StructField(c, StringType(), True) for c in cols])\n",
    "\n",
    "pdf = pd.read_csv(\"C:/Users/pc/Desktop/Epita Msc AIS 1/AIS S2/Spark and python for big data/project 1/customer-churn-sparkml/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaea6dc-0a09-4504-83c3-b2f20a518c21",
   "metadata": {},
   "source": [
    "### 1.3. Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "490814ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 7043\n",
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: long (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: long (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: double (nullable = true)\n",
      " |-- TotalCharges: string (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "|customerID|gender|SeniorCitizen|Partner|Dependents|tenure|PhoneService|   MultipleLines|InternetService|     OnlineSecurity|       OnlineBackup|   DeviceProtection|        TechSupport|        StreamingTV|    StreamingMovies|      Contract|PaperlessBilling|       PaymentMethod|MonthlyCharges|TotalCharges|Churn|\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "|7590-VHVEG|Female|            0|    Yes|        No|     1|          No|No phone service|            DSL|                 No|                Yes|                 No|                 No|                 No|                 No|Month-to-month|             Yes|    Electronic check|         29.85|       29.85|   No|\n",
      "|5575-GNVDE|  Male|            0|     No|        No|    34|         Yes|              No|            DSL|                Yes|                 No|                Yes|                 No|                 No|                 No|      One year|              No|        Mailed check|         56.95|      1889.5|   No|\n",
      "|3668-QPYBK|  Male|            0|     No|        No|     2|         Yes|              No|            DSL|                Yes|                Yes|                 No|                 No|                 No|                 No|Month-to-month|             Yes|        Mailed check|         53.85|      108.15|  Yes|\n",
      "|7795-CFOCW|  Male|            0|     No|        No|    45|          No|No phone service|            DSL|                Yes|                 No|                Yes|                Yes|                 No|                 No|      One year|              No|Bank transfer (au...|          42.3|     1840.75|   No|\n",
      "|9237-HQITU|Female|            0|     No|        No|     2|         Yes|              No|    Fiber optic|                 No|                 No|                 No|                 No|                 No|                 No|Month-to-month|             Yes|    Electronic check|          70.7|      151.65|  Yes|\n",
      "|9305-CDSKC|Female|            0|     No|        No|     8|         Yes|             Yes|    Fiber optic|                 No|                 No|                Yes|                 No|                Yes|                Yes|Month-to-month|             Yes|    Electronic check|         99.65|       820.5|  Yes|\n",
      "|1452-KIOVK|  Male|            0|     No|       Yes|    22|         Yes|             Yes|    Fiber optic|                 No|                Yes|                 No|                 No|                Yes|                 No|Month-to-month|             Yes|Credit card (auto...|          89.1|      1949.4|   No|\n",
      "|6713-OKOMC|Female|            0|     No|        No|    10|          No|No phone service|            DSL|                Yes|                 No|                 No|                 No|                 No|                 No|Month-to-month|              No|        Mailed check|         29.75|       301.9|   No|\n",
      "|7892-POOKP|Female|            0|    Yes|        No|    28|         Yes|             Yes|    Fiber optic|                 No|                 No|                Yes|                Yes|                Yes|                Yes|Month-to-month|             Yes|    Electronic check|         104.8|     3046.05|  Yes|\n",
      "|6388-TABGU|  Male|            0|     No|       Yes|    62|         Yes|              No|            DSL|                Yes|                Yes|                 No|                 No|                 No|                 No|      One year|              No|Bank transfer (au...|         56.15|     3487.95|   No|\n",
      "|9763-GRSKD|  Male|            0|    Yes|       Yes|    13|         Yes|              No|            DSL|                Yes|                 No|                 No|                 No|                 No|                 No|Month-to-month|             Yes|        Mailed check|         49.95|      587.45|   No|\n",
      "|7469-LKBCI|  Male|            0|     No|        No|    16|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      Two year|              No|Credit card (auto...|         18.95|       326.8|   No|\n",
      "|8091-TTVAX|  Male|            0|    Yes|        No|    58|         Yes|             Yes|    Fiber optic|                 No|                 No|                Yes|                 No|                Yes|                Yes|      One year|              No|Credit card (auto...|        100.35|      5681.1|   No|\n",
      "|0280-XJGEX|  Male|            0|     No|        No|    49|         Yes|             Yes|    Fiber optic|                 No|                Yes|                Yes|                 No|                Yes|                Yes|Month-to-month|             Yes|Bank transfer (au...|         103.7|      5036.3|  Yes|\n",
      "|5129-JLPIS|  Male|            0|     No|        No|    25|         Yes|              No|    Fiber optic|                Yes|                 No|                Yes|                Yes|                Yes|                Yes|Month-to-month|             Yes|    Electronic check|         105.5|     2686.05|   No|\n",
      "|3655-SNQYZ|Female|            0|    Yes|       Yes|    69|         Yes|             Yes|    Fiber optic|                Yes|                Yes|                Yes|                Yes|                Yes|                Yes|      Two year|              No|Credit card (auto...|        113.25|     7895.15|   No|\n",
      "|8191-XWSZG|Female|            0|     No|        No|    52|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      One year|              No|        Mailed check|         20.65|     1022.95|   No|\n",
      "|9959-WOFKT|  Male|            0|     No|       Yes|    71|         Yes|             Yes|    Fiber optic|                Yes|                 No|                Yes|                 No|                Yes|                Yes|      Two year|              No|Bank transfer (au...|         106.7|     7382.25|   No|\n",
      "|4190-MFLUW|Female|            0|    Yes|       Yes|    10|         Yes|              No|            DSL|                 No|                 No|                Yes|                Yes|                 No|                 No|Month-to-month|              No|Credit card (auto...|          55.2|      528.35|  Yes|\n",
      "|4183-MYFRB|Female|            0|     No|        No|    21|         Yes|              No|    Fiber optic|                 No|                Yes|                Yes|                 No|                 No|                Yes|Month-to-month|             Yes|    Electronic check|         90.05|      1862.9|   No|\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total rows: {df.count()}\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4262306c-3b00-4da8-9a19-0cc74d21f5d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0338e9e7",
   "metadata": {},
   "source": [
    "## Phase 2: Exploratory Data Analysis (EDA) & Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedcfe8a-1231-4cb3-918a-21941b008c5f",
   "metadata": {},
   "source": [
    "### 2.1. Data Cleaning (Handling Missing Values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a0de19-00c6-4f2a-88eb-ffb7fb2ca7fc",
   "metadata": {},
   "source": [
    "**a) Column count check for Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d066f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column                 Missing Count\n",
      "----------------------------------------\n",
      "customerID                         0\n",
      "gender                             0\n",
      "SeniorCitizen                      0\n",
      "Partner                            0\n",
      "Dependents                         0\n",
      "tenure                             0\n",
      "PhoneService                       0\n",
      "MultipleLines                      0\n",
      "InternetService                    0\n",
      "OnlineSecurity                     0\n",
      "OnlineBackup                       0\n",
      "DeviceProtection                   0\n",
      "TechSupport                        0\n",
      "StreamingTV                        0\n",
      "StreamingMovies                    0\n",
      "Contract                           0\n",
      "PaperlessBilling                   0\n",
      "PaymentMethod                      0\n",
      "MonthlyCharges                     0\n",
      "TotalCharges                       0\n",
      "Churn                              0\n"
     ]
    }
   ],
   "source": [
    "def count_missing(c, dtype):\n",
    "    if isinstance(dtype, StringType):\n",
    "        return F.count(\n",
    "            F.when(\n",
    "                F.col(c).isNull() | (F.trim(F.col(c)) == \"\"), \n",
    "                c\n",
    "            )\n",
    "        ).alias(c)\n",
    "    else:\n",
    "        return F.count(F.when(F.col(c).isNull(), c)).alias(c)\n",
    "\n",
    "missing_counts = df.select([\n",
    "    count_missing(c, dtype) for c, dtype in df.dtypes\n",
    "])\n",
    "\n",
    "missing_data = missing_counts.collect()[0].asDict()\n",
    "\n",
    "print(f\"{'Column':<20} {'Missing Count':>15}\")\n",
    "print(\"-\" * 40)\n",
    "for col_name, count in missing_data.items():\n",
    "    print(f\"{col_name:<20} {count:>15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac05f2f-f929-496a-a1af-2af977412456",
   "metadata": {},
   "source": [
    "**b) Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9740a3ff-3e29-4f33-9141-f154cee82879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows before dropping null TotalCharges: 7043\n",
      "Rows after dropping null TotalCharges: 7032\n",
      "\n",
      "Duplicate customerID count: 0\n"
     ]
    }
   ],
   "source": [
    "# Handle TotalCharges: replace empty strings with null, then cast to Double\n",
    "df = df.withColumn('TotalCharges', \n",
    "                   F.when(F.col('TotalCharges').isin(\"\", \" \"), None)\n",
    "                   .otherwise(F.col('TotalCharges')))\n",
    "\n",
    "# Drop rows with null TotalCharges (11 records)\n",
    "print(f\"\\nRows before dropping null TotalCharges: {df.count()}\")\n",
    "df_clean = df.na.drop(subset=['TotalCharges'])\n",
    "print(f\"Rows after dropping null TotalCharges: {df_clean.count()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate customerID count: {df_clean.count() - df_clean.select('customerID').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0302dbf2-b787-41f7-b5a1-c148e539f487",
   "metadata": {},
   "source": [
    "### 2.2. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96e4a41c-49b1-48f3-b799-98d5e88135ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema after conversion of columns:\n",
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: integer (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: double (nullable = true)\n",
      " |-- TotalCharges: double (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn('TotalCharges', F.col('TotalCharges').cast(DoubleType()))\n",
    "\n",
    "# Cast SeniorCitizen to Integer (here the values are currently \"0\", \"1\")\n",
    "df_clean = df_clean.withColumn('SeniorCitizen', F.col('SeniorCitizen').cast(IntegerType()))\n",
    "\n",
    "# Cast tenure to Integer\n",
    "df_clean = df_clean.withColumn('tenure', F.col('tenure').cast(IntegerType()))\n",
    "\n",
    "# Cast MonthlyCharges to Double\n",
    "df_clean = df_clean.withColumn('MonthlyCharges', F.col('MonthlyCharges').cast(DoubleType()))\n",
    "\n",
    "print(\"Schema after conversion of columns:\")\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58c3d1-71a4-4c32-9a4b-c9b7c050940b",
   "metadata": {},
   "source": [
    "### 2.3. Univariate Analysis (Analyzing Single Variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c863665e-dde4-4044-ad92-f5449726c772",
   "metadata": {},
   "source": [
    "**a) Numerical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a19aa974-ae01-439a-937a-5ca3f92f7684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+\n",
      "|summary|            tenure|   MonthlyCharges|      TotalCharges|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|              7032|             7032|              7032|\n",
      "|   mean|32.421786120591584|64.79820819112626|2283.3004408418656|\n",
      "| stddev|24.545259709263256|30.08597388404984| 2266.771361883145|\n",
      "|    min|                 1|            18.25|              18.8|\n",
      "|    max|                72|           118.75|            8684.8|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n",
      "SeniorCitizen Distribution:\n",
      "+-------------+-----+\n",
      "|SeniorCitizen|count|\n",
      "+-------------+-----+\n",
      "|            0| 5890|\n",
      "|            1| 1142|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# numerical columns summary\n",
    "numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "df_clean.select(numerical_cols).describe().show()\n",
    "\n",
    "# SeniorCitizen belongs to categorical column\n",
    "print(\"SeniorCitizen Distribution:\")\n",
    "df_clean.groupBy('SeniorCitizen').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc24abb-5ea9-4e97-8a9c-e8ba7d044995",
   "metadata": {},
   "source": [
    "**b) Categorical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cade52b-75d4-4415-bb34-ee517dcf9a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn']\n",
      "gender distribution:\n",
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|  Male| 3549|\n",
      "|Female| 3483|\n",
      "+------+-----+\n",
      "\n",
      "Partner distribution:\n",
      "+-------+-----+\n",
      "|Partner|count|\n",
      "+-------+-----+\n",
      "|     No| 3639|\n",
      "|    Yes| 3393|\n",
      "+-------+-----+\n",
      "\n",
      "Dependents distribution:\n",
      "+----------+-----+\n",
      "|Dependents|count|\n",
      "+----------+-----+\n",
      "|        No| 4933|\n",
      "|       Yes| 2099|\n",
      "+----------+-----+\n",
      "\n",
      "Contract distribution:\n",
      "+--------------+-----+\n",
      "|      Contract|count|\n",
      "+--------------+-----+\n",
      "|Month-to-month| 3875|\n",
      "|      Two year| 1685|\n",
      "|      One year| 1472|\n",
      "+--------------+-----+\n",
      "\n",
      "InternetService distribution:\n",
      "+---------------+-----+\n",
      "|InternetService|count|\n",
      "+---------------+-----+\n",
      "|    Fiber optic| 3096|\n",
      "|            DSL| 2416|\n",
      "|             No| 1520|\n",
      "+---------------+-----+\n",
      "\n",
      "PaymentMethod distribution:\n",
      "+--------------------+-----+\n",
      "|       PaymentMethod|count|\n",
      "+--------------------+-----+\n",
      "|    Electronic check| 2365|\n",
      "|        Mailed check| 1604|\n",
      "|Bank transfer (au...| 1542|\n",
      "|Credit card (auto...| 1521|\n",
      "+--------------------+-----+\n",
      "\n",
      "Churn distribution:\n",
      "+-----+-----+\n",
      "|Churn|count|\n",
      "+-----+-----+\n",
      "|   No| 5163|\n",
      "|  Yes| 1869|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [c for c in df_clean.columns if c not in numerical_cols + ['customerID', 'SeniorCitizen']]\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Show value counts for key categorical columns\n",
    "key_categorical = ['gender', 'Partner', 'Dependents', 'Contract', \n",
    "                   'InternetService', 'PaymentMethod', 'Churn']\n",
    "\n",
    "for col in key_categorical:\n",
    "    print(f\"{col} distribution:\")\n",
    "    df_clean.groupBy(col).count().orderBy(F.desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459e000b-1247-429f-adfc-64e2d15bd030",
   "metadata": {},
   "source": [
    "### 2.4. Bivariate Analysis (Analyzing Relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "107bc124-282d-4567-aef2-a2012be1716a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Churn rate by Contract type:\n",
      "+--------------+-----+-------+------------------+\n",
      "|      Contract|total|churned|        churn_rate|\n",
      "+--------------+-----+-------+------------------+\n",
      "|Month-to-month| 3875|   1655| 42.70967741935484|\n",
      "|      One year| 1472|    166|11.277173913043478|\n",
      "|      Two year| 1685|     48|2.8486646884272995|\n",
      "+--------------+-----+-------+------------------+\n",
      "\n",
      "Churn rate by Internet Service:\n",
      "+---------------+-----+-------+------------------+\n",
      "|InternetService|total|churned|        churn_rate|\n",
      "+---------------+-----+-------+------------------+\n",
      "|             No| 1520|    113| 7.434210526315789|\n",
      "|            DSL| 2416|    459|18.998344370860927|\n",
      "|    Fiber optic| 3096|   1297| 41.89276485788114|\n",
      "+---------------+-----+-------+------------------+\n",
      "\n",
      "Tenure statistics by Churn:\n",
      "+-----+------------------+----------+----------+-------------------+-----------------+\n",
      "|Churn|        avg_tenure|min_tenure|max_tenure|avg_monthly_charges|avg_total_charges|\n",
      "+-----+------------------+----------+----------+-------------------+-----------------+\n",
      "|   No| 37.65000968429208|         1|        72|  61.30740848343984|2555.344141003293|\n",
      "|  Yes|17.979133226324237|         1|        72|  74.44133226324236|1531.796094168004|\n",
      "+-----+------------------+----------+----------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Churn rate by Contract type:\")\n",
    "df_clean.groupBy('Contract').agg(\n",
    "    F.count('*').alias('total'),\n",
    "    F.sum(F.when(F.col('Churn') == 'Yes', 1).otherwise(0)).alias('churned'),\n",
    "    (F.sum(F.when(F.col('Churn') == 'Yes', 1).otherwise(0)) / F.count('*') * 100).alias('churn_rate')\n",
    ").show()\n",
    "\n",
    "print(\"Churn rate by Internet Service:\")\n",
    "df_clean.groupBy('InternetService').agg(\n",
    "    F.count('*').alias('total'),\n",
    "    F.sum(F.when(F.col('Churn') == 'Yes', 1).otherwise(0)).alias('churned'),\n",
    "    (F.sum(F.when(F.col('Churn') == 'Yes', 1).otherwise(0)) / F.count('*') * 100).alias('churn_rate')\n",
    ").show()\n",
    "\n",
    "# Tenure statistics by Churn\n",
    "print(\"Tenure statistics by Churn:\")\n",
    "df_clean.groupBy('Churn').agg(\n",
    "    F.avg('tenure').alias('avg_tenure'),\n",
    "    F.min('tenure').alias('min_tenure'),\n",
    "    F.max('tenure').alias('max_tenure'),\n",
    "    F.avg('MonthlyCharges').alias('avg_monthly_charges'),\n",
    "    F.avg('TotalCharges').alias('avg_total_charges')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8427373-cc99-44c4-a504-cd7efac4797e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaeca74",
   "metadata": {},
   "source": [
    "## Phase 3: Data Transformation & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3484e57-e344-4ef7-aa64-c9b5f2fa8ce1",
   "metadata": {},
   "source": [
    "### 3.1. Identify Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e470db35-b391-4dfc-a865-e68432cf27f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to use: ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges']\n",
      "Categorical: ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
      "Numerical: ['tenure', 'MonthlyCharges', 'TotalCharges']\n"
     ]
    }
   ],
   "source": [
    "# we identify feature columns (except for customerID and target)\n",
    "feature_cols = [c for c in df_clean.columns if c not in ['customerID', 'Churn']]\n",
    "\n",
    "# we separate columns into categorical and numerical\n",
    "cat_cols = [c for c in feature_cols if c not in numerical_cols]\n",
    "num_cols = [c for c in feature_cols if c in numerical_cols]\n",
    "\n",
    "print(f\"Features to use: {feature_cols}\")\n",
    "print(f\"Categorical: {cat_cols}\")\n",
    "print(f\"Numerical: {num_cols}\")\n",
    "\n",
    "# we convert the target variable to numeric\n",
    "df_clean = df_clean.withColumn('label', F.when(F.col('Churn') == 'Yes', 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172a3f2-7f40-4364-8010-9fdb2b66faca",
   "metadata": {},
   "source": [
    "### 3.2. Define Pipeline Stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d8745-27da-4547-9f6b-e6686b7fbd4b",
   "metadata": {},
   "source": [
    "**a) Categorical Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0bbd092-f407-42b7-a67d-4cd2791b96a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = []\n",
    "encoders = []\n",
    "indexed_cols = []\n",
    "encoded_cols = []\n",
    "\n",
    "for col in cat_cols:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\", handleInvalid='keep')\n",
    "    encoder = OneHotEncoder(inputCol=f\"{col}_indexed\", outputCol=f\"{col}_encoded\", dropLast=False)\n",
    "    indexers.append(indexer)\n",
    "    encoders.append(encoder)\n",
    "    indexed_cols.append(f\"{col}_indexed\")\n",
    "    encoded_cols.append(f\"{col}_encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001134f8-9cec-47d6-89ed-b819be7cec39",
   "metadata": {},
   "source": [
    "**b) Vector Assembly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de497752-e1e5-4f94-89e3-5401a61e874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine numerical features with encoded categorical features\n",
    "assembler_inputs = num_cols + encoded_cols\n",
    "vector_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol='features_unscaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb2b765-3e55-44ce-96ba-b54ad6fba7f3",
   "metadata": {},
   "source": [
    "**c) Feature Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08e5b918-4023-4588-a08a-d2a4b8662922",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features_unscaled', outputCol='features', \n",
    "                        withStd=True, withMean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150031ad-ccc6-461a-8f80-ede664f94e00",
   "metadata": {},
   "source": [
    "**d) Create Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "823b0533-c664-455e-ab0b-5357dc6aaba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline stages: 16 indexers, 16 encoders, 1 assembler, 1 scaler\n",
      "Total stages in pipeline: 34\n"
     ]
    }
   ],
   "source": [
    "pipeline_stages = indexers + encoders + [vector_assembler, scaler]\n",
    "pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "print(f\"Pipeline stages: {len(indexers)} indexers, {len(encoders)} encoders, 1 assembler, 1 scaler\")\n",
    "print(f\"Total stages in pipeline: {len(pipeline_stages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cbf5db-7ff8-4d7c-bbe1-f32635386fc0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604e5e90",
   "metadata": {},
   "source": [
    "## Phase 4: svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph4-md-lr",
   "metadata": {},
   "source": [
    "### 4.1. Define Logistic Regression & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ph4-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "svm = LinearSVC(\n",
    "    featuresCol='features',\n",
    "    labelCol='label'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph4-md-pipe",
   "metadata": {},
   "source": [
    "### 4.2. Build Full Pipeline (Preprocessing + svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ph4-pipe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM pipeline created with the following stages:\n",
      "  1. StringIndexer\n",
      "  2. StringIndexer\n",
      "  3. StringIndexer\n",
      "  4. StringIndexer\n",
      "  5. StringIndexer\n",
      "  6. StringIndexer\n",
      "  7. StringIndexer\n",
      "  8. StringIndexer\n",
      "  9. StringIndexer\n",
      "  10. StringIndexer\n",
      "  11. StringIndexer\n",
      "  12. StringIndexer\n",
      "  13. StringIndexer\n",
      "  14. StringIndexer\n",
      "  15. StringIndexer\n",
      "  16. StringIndexer\n",
      "  17. OneHotEncoder\n",
      "  18. OneHotEncoder\n",
      "  19. OneHotEncoder\n",
      "  20. OneHotEncoder\n",
      "  21. OneHotEncoder\n",
      "  22. OneHotEncoder\n",
      "  23. OneHotEncoder\n",
      "  24. OneHotEncoder\n",
      "  25. OneHotEncoder\n",
      "  26. OneHotEncoder\n",
      "  27. OneHotEncoder\n",
      "  28. OneHotEncoder\n",
      "  29. OneHotEncoder\n",
      "  30. OneHotEncoder\n",
      "  31. OneHotEncoder\n",
      "  32. OneHotEncoder\n",
      "  33. VectorAssembler\n",
      "  34. StandardScaler\n",
      "  35. LinearSVC\n"
     ]
    }
   ],
   "source": [
    "svm_pipeline = Pipeline(stages=indexers + encoders + [vector_assembler, scaler, svm])\n",
    "\n",
    "print('SVM pipeline created with the following stages:')\n",
    "for i, stage in enumerate(svm_pipeline.getStages()):\n",
    "    print(f'  {i+1}. {stage.__class__.__name__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph4-md-split",
   "metadata": {},
   "source": [
    "### 4.3. Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ph4-split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set : 5646 records\n",
      "Test set     : 1386 records\n",
      "\n",
      "Class distribution in training set:\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0| 4153|\n",
      "|    1| 1493|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f'Training set : {train_df.count()} records')\n",
    "print(f'Test set     : {test_df.count()} records')\n",
    "print('\\nClass distribution in training set:')\n",
    "train_df.groupBy('label').count().orderBy('label').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph4-md-grid",
   "metadata": {},
   "source": [
    "### 4.4. Define Hyperparameter Grid with ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ph4-grid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM ParamGrid built — 6 combinations\n",
      "  Combo  1: {'regParam': 0.01, 'maxIter': 10}\n",
      "  Combo  2: {'regParam': 0.01, 'maxIter': 50}\n",
      "  Combo  3: {'regParam': 0.1, 'maxIter': 10}\n",
      "  Combo  4: {'regParam': 0.1, 'maxIter': 50}\n",
      "  Combo  5: {'regParam': 0.5, 'maxIter': 10}\n",
      "  Combo  6: {'regParam': 0.5, 'maxIter': 50}\n"
     ]
    }
   ],
   "source": [
    "svm_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(svm.regParam, [0.01, 0.1, 0.5])\n",
    "    .addGrid(svm.maxIter,  [10, 50])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(f'SVM ParamGrid built — {len(svm_param_grid)} combinations')\n",
    "for i, params in enumerate(svm_param_grid):\n",
    "    combo = {p.name: v for p, v in params.items()}\n",
    "    print(f'  Combo {i+1:>2}: {combo}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph4-md-cv",
   "metadata": {},
   "source": [
    "### 4.5. Cross-Validation Setup (5-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ph4-cv",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol='label',\n",
    "    rawPredictionCol='rawPrediction',\n",
    "    metricName='areaUnderROC'\n",
    ")\n",
    "\n",
    "svm_cross_val = CrossValidator(\n",
    "    estimator=svm_pipeline,\n",
    "    estimatorParamMaps=svm_param_grid,\n",
    "    evaluator=cv_evaluator,\n",
    "    numFolds=5,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph4-md-train",
   "metadata": {},
   "source": [
    "### 4.6. Train with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ph4-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average AUC-ROC per hyperparameter combination:\n",
      "Combo    regParam   maxIter    AUC-ROC\n",
      "----------------------------------------\n",
      "1        0.01       10          0.8349\n",
      "2        0.01       50          0.8348\n",
      "3        0.1        10          0.8349\n",
      "4        0.1        50          0.8363\n",
      "5        0.5        10          0.8334\n",
      "6        0.5        50          0.8347\n",
      "\n",
      "Best combo #4: {'regParam': 0.1, 'maxIter': 50}  →  AUC = 0.8363\n"
     ]
    }
   ],
   "source": [
    "svm_cv_model = svm_cross_val.fit(train_df)\n",
    "\n",
    "print('\\nAverage AUC-ROC per hyperparameter combination:')\n",
    "print(f\"{'Combo':<8} {'regParam':<10} {'maxIter':<9} {'AUC-ROC':>8}\")\n",
    "print('-' * 40)\n",
    "\n",
    "for i, (params, score) in enumerate(zip(svm_param_grid, svm_cv_model.avgMetrics)):\n",
    "    combo = {p.name: v for p, v in params.items()}\n",
    "    print(f\"{i+1:<8} {combo['regParam']:<10} {combo['maxIter']:<9} {score:>8.4f}\")\n",
    "\n",
    "best_svm_score  = max(svm_cv_model.avgMetrics)\n",
    "best_svm_idx    = svm_cv_model.avgMetrics.index(best_svm_score)\n",
    "best_svm_params = {p.name: v for p, v in svm_param_grid[best_svm_idx].items()}\n",
    "print(f'\\nBest combo #{best_svm_idx+1}: {best_svm_params}  →  AUC = {best_svm_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph4-md-eval",
   "metadata": {},
   "source": [
    "### 4.7. Evaluate Best Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ph4-eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Evaluation\n",
      "----------------------------------------\n",
      "  Accuracy          : 0.7893\n",
      "  F1 Score          : 0.7808\n",
      "  Weighted Precision: 0.7785\n",
      "  Weighted Recall   : 0.7893\n",
      "  AUC-ROC           : 0.8346\n",
      "\n",
      "Confusion Matrix:\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0|  904|\n",
      "|    0|       1.0|  106|\n",
      "|    1|       0.0|  186|\n",
      "|    1|       1.0|  190|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_predictions = svm_cv_model.transform(test_df)\n",
    "\n",
    "svm_acc = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy').evaluate(svm_predictions)\n",
    "svm_f1 = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1').evaluate(svm_predictions)\n",
    "svm_prec = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='weightedPrecision').evaluate(svm_predictions)\n",
    "svm_rec = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='weightedRecall').evaluate(svm_predictions)\n",
    "svm_auc = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderROC').evaluate(svm_predictions)\n",
    "\n",
    "print(\"\\nSVM Evaluation\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Accuracy          : {svm_acc:.4f}\")\n",
    "print(f\"  F1 Score          : {svm_f1:.4f}\")\n",
    "print(f\"  Weighted Precision: {svm_prec:.4f}\")\n",
    "print(f\"  Weighted Recall   : {svm_rec:.4f}\")\n",
    "print(f\"  AUC-ROC           : {svm_auc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "svm_predictions.groupBy('label', 'prediction').count().orderBy('label', 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph5-sep",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph5-title",
   "metadata": {},
   "source": [
    "## Phase 5: Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph5-md-import",
   "metadata": {},
   "source": [
    "### 5.1. Add RandomForestClassifier to the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ph5-import",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF pipeline created with the following stages:\n",
      "  1. StringIndexer\n",
      "  2. StringIndexer\n",
      "  3. StringIndexer\n",
      "  4. StringIndexer\n",
      "  5. StringIndexer\n",
      "  6. StringIndexer\n",
      "  7. StringIndexer\n",
      "  8. StringIndexer\n",
      "  9. StringIndexer\n",
      "  10. StringIndexer\n",
      "  11. StringIndexer\n",
      "  12. StringIndexer\n",
      "  13. StringIndexer\n",
      "  14. StringIndexer\n",
      "  15. StringIndexer\n",
      "  16. StringIndexer\n",
      "  17. OneHotEncoder\n",
      "  18. OneHotEncoder\n",
      "  19. OneHotEncoder\n",
      "  20. OneHotEncoder\n",
      "  21. OneHotEncoder\n",
      "  22. OneHotEncoder\n",
      "  23. OneHotEncoder\n",
      "  24. OneHotEncoder\n",
      "  25. OneHotEncoder\n",
      "  26. OneHotEncoder\n",
      "  27. OneHotEncoder\n",
      "  28. OneHotEncoder\n",
      "  29. OneHotEncoder\n",
      "  30. OneHotEncoder\n",
      "  31. OneHotEncoder\n",
      "  32. OneHotEncoder\n",
      "  33. VectorAssembler\n",
      "  34. StandardScaler\n",
      "  35. RandomForestClassifier\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_pipeline = Pipeline(stages=indexers + encoders + [vector_assembler, scaler, rf])\n",
    "\n",
    "print(\"RF pipeline created with the following stages:\")\n",
    "for i, stage in enumerate(rf_pipeline.getStages()):\n",
    "    print(f\"  {i+1}. {stage.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph5-md-grid",
   "metadata": {},
   "source": [
    "### 5.2. Define Hyperparameter Grid with ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ph5-grid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF ParamGrid built — 12 hyperparameter combinations\n",
      "  Combo  1: {'numTrees': 50, 'maxDepth': 4, 'minInstancesPerNode': 1}\n",
      "  Combo  2: {'numTrees': 50, 'maxDepth': 4, 'minInstancesPerNode': 2}\n",
      "  Combo  3: {'numTrees': 50, 'maxDepth': 6, 'minInstancesPerNode': 1}\n",
      "  Combo  4: {'numTrees': 50, 'maxDepth': 6, 'minInstancesPerNode': 2}\n",
      "  Combo  5: {'numTrees': 50, 'maxDepth': 8, 'minInstancesPerNode': 1}\n",
      "  Combo  6: {'numTrees': 50, 'maxDepth': 8, 'minInstancesPerNode': 2}\n",
      "  Combo  7: {'numTrees': 70, 'maxDepth': 4, 'minInstancesPerNode': 1}\n",
      "  Combo  8: {'numTrees': 70, 'maxDepth': 4, 'minInstancesPerNode': 2}\n",
      "  Combo  9: {'numTrees': 70, 'maxDepth': 6, 'minInstancesPerNode': 1}\n",
      "  Combo 10: {'numTrees': 70, 'maxDepth': 6, 'minInstancesPerNode': 2}\n",
      "  Combo 11: {'numTrees': 70, 'maxDepth': 8, 'minInstancesPerNode': 1}\n",
      "  Combo 12: {'numTrees': 70, 'maxDepth': 8, 'minInstancesPerNode': 2}\n"
     ]
    }
   ],
   "source": [
    "rf_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees,             [50, 70])\n",
    "    .addGrid(rf.maxDepth,             [4, 6, 8])\n",
    "    .addGrid(rf.minInstancesPerNode,  [1, 2])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(f\"RF ParamGrid built — {len(rf_param_grid)} hyperparameter combinations\")\n",
    "for i, params in enumerate(rf_param_grid):\n",
    "    combo = {p.name: v for p, v in params.items()}\n",
    "    print(f\"  Combo {i+1:>2}: {combo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph5-md-cv",
   "metadata": {},
   "source": [
    "### 5.3. Cross-Validation Setup (5-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ph5-cv",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cross_val = CrossValidator(\n",
    "    estimator=rf_pipeline,\n",
    "    estimatorParamMaps=rf_param_grid,\n",
    "    evaluator=cv_evaluator,  # Reusing the evaluator defined in Phase 4\n",
    "    numFolds=5,\n",
    "    seed=42,\n",
    "    parallelism=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph5-md-train",
   "metadata": {},
   "source": [
    "### 5.4. Train with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ph5-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average AUC-ROC per hyperparameter combination:\n",
      "Combo    numTrees   maxDepth   minInst   AUC-ROC\n",
      "--------------------------------------------------\n",
      "1        50         4          1          0.8345\n",
      "2        50         4          2          0.8343\n",
      "3        50         6          1          0.8405\n",
      "4        50         6          2          0.8404\n",
      "5        50         8          1          0.8421\n",
      "6        50         8          2          0.8431\n",
      "7        70         4          1          0.8340\n",
      "8        70         4          2          0.8334\n",
      "9        70         6          1          0.8404\n",
      "10       70         6          2          0.8403\n",
      "11       70         8          1          0.8425\n",
      "12       70         8          2          0.8431\n",
      "\n",
      "Best combo #12: {'numTrees': 70, 'maxDepth': 8, 'minInstancesPerNode': 2}  →  AUC = 0.8431\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf_cv_model = rf_cross_val.fit(train_df)\n",
    "\n",
    "print(\"\\nAverage AUC-ROC per hyperparameter combination:\")\n",
    "print(f\"{'Combo':<8} {'numTrees':<10} {'maxDepth':<10} {'minInst':<8} {'AUC-ROC':>8}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, (params, score) in enumerate(zip(rf_param_grid, rf_cv_model.avgMetrics)):\n",
    "    combo = {p.name: v for p, v in params.items()}\n",
    "    print(f\"{i+1:<8} {combo['numTrees']:<10} {combo['maxDepth']:<10} {combo['minInstancesPerNode']:<8} {score:>8.4f}\")\n",
    "\n",
    "best_rf_score = max(rf_cv_model.avgMetrics)\n",
    "best_rf_idx   = rf_cv_model.avgMetrics.index(best_rf_score)\n",
    "best_rf_params = {p.name: v for p, v in rf_param_grid[best_rf_idx].items()}\n",
    "print(f\"\\nBest combo #{best_rf_idx+1}: {best_rf_params}  →  AUC = {best_rf_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ph5-md-eval",
   "metadata": {},
   "source": [
    "### 5.5. Evaluate Best Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ph5-eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Evaluation\n",
      "----------------------------------------\n",
      "  Accuracy          : 0.7951\n",
      "  F1 Score          : 0.7811\n",
      "  Weighted Precision: 0.7828\n",
      "  Weighted Recall   : 0.7951\n",
      "  AUC-ROC           : 0.8461\n",
      "\n",
      "Confusion Matrix:\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0|  929|\n",
      "|    0|       1.0|   81|\n",
      "|    1|       0.0|  203|\n",
      "|    1|       1.0|  173|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions = rf_cv_model.transform(test_df)\n",
    "\n",
    "rf_acc = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy').evaluate(rf_predictions)\n",
    "rf_f1 = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1').evaluate(rf_predictions)\n",
    "rf_prec = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='weightedPrecision').evaluate(rf_predictions)\n",
    "rf_rec = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='weightedRecall').evaluate(rf_predictions)\n",
    "rf_auc = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderROC').evaluate(rf_predictions)\n",
    "\n",
    "print(\"\\nRandom Forest Evaluation\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Accuracy          : {rf_acc:.4f}\")\n",
    "print(f\"  F1 Score          : {rf_f1:.4f}\")\n",
    "print(f\"  Weighted Precision: {rf_prec:.4f}\")\n",
    "print(f\"  Weighted Recall   : {rf_rec:.4f}\")\n",
    "print(f\"  AUC-ROC           : {rf_auc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "rf_predictions.groupBy('label', 'prediction').count().orderBy('label', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76bc28b9-cd99-4a45-8eff-bfcd5aae5f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession stopped.\n"
     ]
    }
   ],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "print(\"SparkSession stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065752da-2bb6-4a6e-9ed0-572f365e297d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
